---
title: "Measurement invariance of outcome measures"
author: "Stef Meliss"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

options(scipen = 999)
# empty work space
rm(list = ls())

# define directory
dir <- getwd()
dir <- gsub("/02_analysis", "", dir)

# load libraries
library(kableExtra)
library(dplyr)

library(lavaan)
library(semPlot)
library(semptools)
library(semTools)

# read in functions
devtools::source_url("https://github.com/stefaniemeliss/Global_Teaching_InSights/blob/main/functions.R?raw=TRUE")
devtools::source_url("https://github.com/stefaniemeliss/MAGMOT/blob/master/functions/rbindcolumns.R?raw=TRUE")

# load in data #
stud <- read.csv(file.path(dir, "data_raw", "GTI-Student-Data.csv"))


# replace all 9999 with NA #
stud <- stud %>%
  mutate(across(where(is.integer), ~na_if(., 9999))) %>% # missing
  mutate(across(where(is.integer), ~na_if(., 9998))) %>% # multiple responses
  mutate(across(where(is.integer), ~na_if(., 9997))) # Illegible response

# remove Madrid
stud <- subset(stud, stud$COUNTRY != "Madrid")

# recode items
stud[,paste0("SQB11", LETTERS[c(1, 2, 3)])] <- 5 - stud[,paste0("SQB11", LETTERS[c(1, 2, 3)])]

```

```{r, echo = F, results='asis'}
# define constructs measures
constructs <- c(
  "Self-concept in mathematics",
  "Self-concept in mathematics",
  "Personal interest in mathematics",
  "Personal interest in mathematics",
  "General self-efficacy in mathematics",
  "General self-efficacy in mathematics",
  "Task-specific self-efficacy in mathematics",
  "Task-specific self-efficacy in mathematics"
)

# determine unique variable names
vars <- c(
  # self-concept
  "SELFCON_baseline",
  "SELFCON_posttest",
  # personal interest
  "PINT_baseline",
  "PINT_posttest",
  # general self-efficacy
  "GENSELFEFF_baseline",
  "GENSELFEFF_posttest",
  # MATCHED task-specific self- efficacy 
  "EFFICACY_MATCHED_baseline",
  "EFFICACY_MATCHED_posttest"
)

# add timepoint
timepoints <- rep_len(c("T1", "T2"), length.out = length(vars))

# create nice outcome names
vars_out <- paste(constructs, timepoints)

# determine items that are summed up to factor
scales <- c(
  # self-concept
  paste0("SQA06", LETTERS[1:6],collapse = " + "),
  paste0("SQB01", LETTERS[1:6],collapse = " + "),
  # personal interest
  paste0("SQA14", LETTERS[1:3],collapse = " + "),
  paste0("SQB03", LETTERS[1:3],collapse = " + "),
  # general self-efficacy
  paste0("SQA15", LETTERS[1:5],collapse = " + "),
  paste0("SQB02", LETTERS[1:5],collapse = " + "),
  # MATCHED task-specific self- efficacy 
  paste0("SQA16", LETTERS[6:10],"2", collapse = " + "),
  paste0("SQB07C", LETTERS[1:5], collapse = " + ")
)


# combine to df for checking
df <- data.frame(vars, constructs, timepoints, vars_out, scales)
df$models <- paste(timepoints, "=~", scales)


```

```{r working_chunk, echo=FALSE, eval=F}
options(knitr.kable.NA = '')
# debug
# vars <- vars[2]

# loop over all variables
for (v in 1:length(vars)) {
  
  # lavaan model formulation
  model <- df$models[v]
  
  # baseline model fit in each country #
  
  countries <- unique(stud$COUNTRY)
  
  df_conf <- data.frame(Country = countries,
                        ChiSq = NA,
                        Df = NA,
                        p.Value = NA,
                        CFI = NA,
                        TLI = NA,
                        RMSEA = NA)
  
  
  for (c in 1:length(countries)) {
    
    # compute CFA in each country
    tmp_fit <-lavaan::cfa(model, missing = "pairwise", warn = FALSE, data = stud[stud$COUNTRY == countries[c], ])
    
    # put summary in object
    tmp <- summary(tmp_fit, fit.measures = TRUE)
    
    # extract relevant information from tmp
    tmpp <- tmp$pe[! grepl("SQ", tmp$pe$lhs) & grepl("SQ", tmp$pe$rhs), ]
    
    # add country information
    tmpp$Country <- countries[c]
    
    # use relevant cols only
    tmpp <- tmpp[, c("rhs", "Country", "est", "pvalue")]
    tmpp <- tmpp[, c("rhs", "Country", "pvalue")]
    tmpp$pvalue <- tmpp$pvalue < 0.05 # true if item loads significantly on factor
    
    # add to data frame
    df_conf$ChiSq[df_conf$Country == countries[c]] <- tmp$fit["chisq"]
    df_conf$Df[df_conf$Country == countries[c]] <- tmp$fit["df"]
    df_conf$p.Value[df_conf$Country == countries[c]] <- tmp$fit["pvalue"]
    df_conf$CFI[df_conf$Country == countries[c]] <- tmp$fit["cfi"]
    df_conf$TLI[df_conf$Country == countries[c]] <- tmp$fit["tli"]
    df_conf$RMSEA[df_conf$Country == countries[c]] <- tmp$fit["rmsea"]
    
    if (c == 1) {
      loadings <- tmpp
    } else {
      loadings <- rbind(loadings, tmpp)
    }
    
  }
  
  # reshape
  tmp4 <- reshape(loadings, idvar = "rhs", timevar = "Country", direction = "wide")
  
  # create a variable that checks whether the loading is significant across all countries
  # this is done by computing the SD of the boolean variable pvalue per country
  tmp4$sd_loadings_sig <- apply(tmp4[, -1], MARGIN = 1, FUN = sd, na.rm = T)
  
  
  # round df
  df_conf[-1] <- round(df_conf[-1], 3)
  
  # get worst fit indices
  afi_country <- data.frame(model = paste0("Within country range: ", sum(tmp4$sd_loadings_sig, na.rm = T)),
                            ChiSq = paste0("[", min(df_conf$ChiSq), "; ", max(df_conf$ChiSq), "] (", unique(df_conf$Df), ")"),
                            CFI = paste0("[", min(df_conf$CFI), "; ", max(df_conf$CFI), "]"),
                            TLI = paste0("[", min(df_conf$TLI), "; ", max(df_conf$TLI), "]"),
                            RMSEA = paste0("[", min(df_conf$RMSEA), "; ", max(df_conf$RMSEA), "]"))
  
  
  # configural invariance #
  
  # fit CFA to test for conigural invariance
  Configural <- lavaan::cfa(model, missing = "pairwise", warn = FALSE, data = stud, group = "COUNTRY")
  
  # put summary in object
  tmp <- summary(Configural, fit.measures = TRUE)
  
  # safe alternative fit indices (AFI) in df
  afi <- data.frame(Configural = tmp$fit)
  afi$index <- row.names(afi)
  row.names(afi) <- NULL
  
  # metric invariance #
  
  Metric <-lavaan::cfa(model, missing = "pairwise", warn = FALSE, data = stud, group = "COUNTRY", group.equal = c("loadings"))
  
  # put summary in object
  tmp <- summary(Metric, fit.measures = TRUE)
  
  # safe alternative fit indices (AFI) in df
  tmp_afi <- data.frame(Metric = tmp$fit)
  tmp_afi$index <- row.names(tmp_afi)
  row.names(tmp_afi) <- NULL
  
  # merge baseline and config
  afi <- merge(afi, tmp_afi, by = "index")
  
  # create table for reporting tests of measurement invariance #
  
  # transpose AFIs
  fit <- afi[-1] %>% 
    t() %>% as.data.frame() %>% setNames(afi[,1]) %>% round(digits = 3)
  # add column that indicates model
  fit$model <- row.names(fit)
  row.names(fit) <- NULL
  
  # re-format values
  fit$pval <-ifelse(fit$pvalue < .001, "***", 
                    ifelse(fit$pvalue < .05, "*",
                           ifelse(fit$pvalue >= .05, "n.s.", NA)))
  fit$chisq <- paste0(fit$chisq, fit$pval, " (", fit$df, ")")
  fit$RMSEA <- paste0(fit$rmsea, " [", fit$rmsea.ci.lower, "; ", fit$rmsea.ci.upper, "]")
  #fit$RMSEA.scaled <- paste0(fit$rmsea.scaled, " [", fit$rmsea.ci.lower.scaled, "; ", fit$rmsea.ci.upper.scaled, "]")
  
  # select relevant columns
  fit <- fit[, c("model", 
                 "chisq", 
                 "cfi", 
                 "tli", 
                 "rmsea"
                 #"RMSEA"
  )]
  
  names(fit) <- c("model", 
                  "ChiSq", 
                  "CFI", 
                  "TLI", 
                  "RMSEA")
  
  # add country fit
  fit <- rbind.all.columns(afi_country, fit)
  
  # compare model fit #
  comp <- compareFit(list(Configural, Metric))
  
  # get chi square model comparison etc
  chi2 <- slot(comp, "nested")
  # remove empty row
  chi2 <- chi2[!is.na(chi2$`Df diff`),]
  chi2 <- round(chi2, digits = 3)
  chi2$model <- "Metric"
  # re-format values
  chi2$pval <-ifelse(chi2$`Pr(>Chisq)` < .001, "***", 
                     ifelse(chi2$`Pr(>Chisq)` < .05, "*",
                            ifelse(chi2$`Pr(>Chisq)` >= .05, "n.s.", NA)))
  chi2$diff.chisq <- paste0(chi2$`Chisq diff`, chi2$pval, " (", chi2$`Df diff`, ")")
  
  # select relevant columns
  chi2 <- chi2[, c("model",
                   "diff.chisq")]
  row.names(chi2) <- NULL
  
  # change in model fit indices
  tmp <- slot(comp, "fit.diff")
  tmp <- round(tmp, digits = 3)
  tmp$model <- "Metric"
  row.names(tmp) <- NULL
  
  # re-format values
  tmp$RMSEA <- paste0(tmp$rmsea, " [", tmp$rmsea.ci.lower, "; ", tmp$rmsea.ci.upper, "]")
  #tmp$RMSEA.scaled <- paste0(tmp$rmsea.scaled, " [", tmp$rmsea.ci.lower.scaled, "; ", tmp$rmsea.ci.upper.scaled, "]")
  
  
  # select relevant columns
  tmp <- tmp[, c("model", 
                 "cfi", 
                 "rmsea"
  )]
  
  # change column names
  names(tmp) <- c("model", 
                  "Δ CFI", 
                  "Δ RMSEA"
  )
  
  # combine change in chi square and alternative fit indices
  tmp <- merge(chi2, tmp, by = "model")
  
  # combine all fit info related to MI in one table
  out <- merge(fit, tmp, by = "model", all = T)
  
  # re-order rows
  out <- out[c(3,1,2),]
  
  # fit baseline CFA model ORDINAL #
  Ordinal <-lavaan::cfa(model, ordered = T, missing = "pairwise", warn = FALSE, data = stud)
  
  # put summary in object
  tmp <- summary(Ordinal, fit.measures = TRUE)
  
  # safe alternative fit indices (AFI) in df
  afi <- data.frame(Ordinal = tmp$fit)
  afi$index <- row.names(afi)
  row.names(afi) <- NULL
  
  # transpose AFIs
  fit <- afi[-2] %>% 
    t() %>% as.data.frame() %>% setNames(afi[,2]) %>% round(digits = 3)
  # add column that indicates model
  fit$model <- row.names(fit)
  row.names(fit) <- NULL
  
  # # re-format values: ROBUST
  # fit$pval <-ifelse(fit$pvalue < .001, "***",
  #                   ifelse(fit$pvalue < .05, "*",
  #                          ifelse(fit$pvalue >= .05, "n.s.", NA)))
  # fit$chisq <- paste0(fit$chisq, fit$pval, " (", fit$df, ")")
  # fit$RMSEA <- paste0(fit$rmsea.robust, " [", fit$rmsea.ci.lower.robust, "; ", fit$rmsea.ci.upper.robust, "]")
  # 
  # # select relevant columns
  # fit <- fit[, c("model",
  #                "chisq", # this is scaled
  #                "cfi.robust",
  #                "tli.robust",
  #                #"RMSEA",
  #                "rmsea.robust"
  # )]
  
  # re-format values: SCALED
  fit$pval <-ifelse(fit$pvalue.scaled < .001, "***",
                    ifelse(fit$pvalue.scaled < .05, "*",
                           ifelse(fit$pvalue.scaled >= .05, "n.s.", NA)))
  fit$chisq <- paste0(fit$chisq.scaled, fit$pval, " (", fit$df.scaled, ")")
  fit$RMSEA <- paste0(fit$rmsea.scaled, " [", fit$rmsea.ci.lower.scaled, "; ", fit$rmsea.ci.upper.scaled, "]")
  
  # select relevant columns
  fit <- fit[, c("model",
                 "chisq", # this is scaled
                 "cfi.scaled",
                 "tli.scaled",
                 #"RMSEA",
                 "rmsea.scaled"
  )]
  
  
  # rename columns
  names(fit) <- c("model", 
                  "ChiSq", 
                  "CFI", 
                  "TLI", 
                  "RMSEA")
  
  # combine with other output
  out <- rbind.all.columns(out, fit)
  
  # two outputs will be saved: one that is plotted in the markdown, hence needs variable info as column
  # one for csv export to create apa style table, hence needs variable as row
  
  # APA: put row on top that contains the variable name in model column
  tmp_apa <- rbind(NA, out)
  tmp_apa$model[1] <- vars_out[v] # add var
  
  # markdown: add column with variable
  tmp_md <- out
  tmp_md$var <- vars_out[v] # add var
  tmp_md <- tmp_md %>%
    relocate(var) # move first
  
  # combine all data in one output for extraction
  if (v == 1) {
    apa <- tmp_apa
    md <- tmp_md
  } else {
    apa <- rbind(apa, tmp_apa)
    md <- rbind(md, tmp_md)
  }
  
}


# declare factor to help with ordering
md$var <- factor(md$var, levels = c(df$vars_out))
# md <- md[order(md$var),]

# define thresholds for highlighting
col.cfi <- which(md$CFI < .95)
col.tli <- which(md$TLI < .95)
col.rmsea <- which(md$RMSEA > .1)
col.red <- which(md$`Δ CFI` < -.02 | md$`Δ RMSEA` > .03)


# print table
kbl(md[-1], row.names = F,
    caption = "Report of tests of measurement invariance") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), fixed_thead = T) %>%
  #row_spec(col.cfi, bold = T, color = blue) %>%
  #row_spec(col.tli, bold = T, color = blue) %>%
  #row_spec(col.rmsea, bold = T, color = blue) %>%
  #row_spec(col.red, bold = T, color = red) %>%
  pack_rows(index = table(md$var)) %>%
  add_footnote("Note. For the baseline model that treats the item as ordinal, scaled overall fit measures are reported.") %>%
  print()
cat("\n")

# save apa file
write.csv(apa, file = filename, row.names = F, na = "")


```

The property of a scale to measure the underlying latent variable(s) equally across different groups (e.g., different countries) is referred to as measurement invariance (MI; Rutkowski & Svetina, 2014). MI (or equivalence) assesses and compares the psychometric properties of a scales across groups to demonstrate the construct has the same meaning across groups (Putnick & Bornstein, 2016). Different aspects of MI can be distinguished (Horn & McArdle, 1992; Putnick & Bornstein, 2016; Vandenberg & Lance, 2000). Configural (or model form) MI assumes that each specified group has the same pattern of factor loadings. Metric (or weak factorial) MI additionally requires the loadings of the items on a factor to be invariant across groups. Scalar (or strong factorial) MI additionally constraints the intercepts of the item’s loading on the factor(s) to be invariant across groups. Residual (or strict) MI further restricts the items’ residuals and unique variances to be equal across groups. To test for MI, a structural equation modelling (SEM) framework can be adopted, most frequently using multiple group confirmatory factor analysis (MG-CFA). Constraints are added to the MG-CFA model in a stepwise fashion and MI is tested by evaluating how well the specified model fits the data in comparison to the lesser constrained nested model (Hirschfeld et al., 2014; Putnick & Bornstein, 2016).  

In the context of this project, it was necessary to establish metric MI to allow us to pool data collected in different countries/economies that participated in the GTI study. It was not necessary to establish scalar invariance as we are not interested in comparing means across groups. While various guidelines exist to establish measurement invariance (e.g., Hirschfeld et al., 2014; Putnick & Bornstein, 2016), they may not be applicable to international large-scale assessments (ILSAs) because ILSAs often compare relatively large numbers of groups with large sample sizes (Rutkowski & Svetina, 2014) as decision criteria were often validated for two-group investigations (Chen, 2007; Cheung & Rensvold, 2002; French & Finch, 2006). We hence here apply the evaluation steps and decision criteria that have specifically been proposed in the context of ILSAs (Rutkowski & Svetina, 2014): (1) fit configural MG-CFA model to the data of all countries/economies, (2) assess overall model fit, (3) fit metric MG-CFA model to the data of all countries/economies, (4) assess overall model fit and (5) access relative (or comparative) model fit. To access the overall model fit in steps (2) and (4), the χ2 (chi-square) test of model fit as well as alternative fit indices were used, i.e., Comparative Fit Index (CFI), Tucker–Lewis Index (TLI) and Root Mean Square Error of Approximation (RMSEA). For the overall model fit to be acceptable, CFI and TLI needed to be larger than .95 and RMSEA smaller than or equal to .10. To examine relative model fit, we used χ2 difference test, ΔCFI and ΔRMSEA were used. Here, we applied a cutoff of -.020 to the ΔCFI values and a cutoff of .030 to the ΔRMSEA values.  

It is important to note that the cut-offs proposed by Rutkowski & Svetina (2014) were derived using models that treat the 4-point Likert scale items as continuous rather than ordered categorical (ordinal). As discussed by the authors, this choice is theoretically not best practice but reflects current state of practice in the context of establishing MI in ILSAs. As an additional step upon establishing metric MI, we specified a baseline CFA model pooling the data from all countries/economies and treating the items as ordinal and evaluated the overall model fit. **ADD COMMENT re DWLS estimator and that *scaled* statistics were hence reported reported**  

All MI analyses were conducted using the R package lavaan (version 0.6-17; Rosseel, 2012). Missing values were handled using pairwise deletion. To fit the CFA models, the scale specifications suggested by the GTI study authors were used for all scales other than task-specific self-efficacy in mathematics were we only included the subset of matching items used in student pre- and post-questionnaire were included (see Table S[XXX]). For each non-cognitive outcome measure of interest, we defined a model where the items load on a single factor reflecting the construct as specified in the [GTI technical report](https://web-archive.oecd.org/2020-11-15/569819-GTI-TechReport-AnnexD5.pdf).

> factor =~ item_1 + item_2 + ... + item_n  

The models for each construct were fitted separately for the student pre- and post-questionnaire. The models were initially fitted on all available data and overall and relative model fit indices were extracted. Please note that for the any three-item scale (e.g., personal interest in mathematics), the configural model was just identified and hence the model fit could not be evaluated due to a lack of degrees of freedom. As such, the overall model fit indices of the configural model indicate perfect model fit because the model was directly derived from the data. Likewise, the relative model fit cannot be interpreted. For any three-item scales, metric MI was assumed if the overall model fit of the metric model was acceptable. As shown in Table S[XXX], metric MI could not be assumed across all scales when using data from all seven countries/economies.   


### Results when including data from all 7 countries

```{r, echo = F, results='asis', fig.align='center', warning=FALSE}
# determine file name for tabel in apa
filename <- "01_mi_all.csv"
# run chunk
<<working_chunk>>
```

In a following step, we iteratively removed countries and repeated the MI analyses until we identified subsets of countries for which metric MI was found. After excluding data from Shanghai (China), we found that all scales showed metric MI (see Table S[XXX]).   

### Results when including data from all 6 selected countries

```{r, echo = F, results='asis', fig.align='center', warning=FALSE}
# remove countries #

# countries to exclude
countries <- c("Shanghai")

# remove data from country
stud <- stud %>%
  filter(!COUNTRY %in% countries)

# determine file name for tabel in apa
filename <- "01_mi_select.csv"

# run chunk
<<working_chunk>>

```

##### Confirmatory factor analysis (CFA) plots for the non-cognitive outcomes  

```{r, echo = F, results='asis', fig.align='center', warning=FALSE}
# create all CFA plots #

# format input data
df$vars_tmp <- tolower(gsub("_baseline|_posttest", "", df$vars))

# long to wide
df_wide <- df %>%
  select(vars_tmp, constructs, timepoints, models) %>%
  tidyr::pivot_wider(names_from = timepoints, values_from = models)

# for each construct
for (c in 1:length(df_wide$constructs)) {
  
  # create plot
  create_cfa_plot_longitudinal(model_baseline = df_wide$T1[c],
                               model_posttest = df_wide$T2[c],
                               construct_name = paste(LETTERS[c], df_wide$constructs[c] ),
                               filename = paste0("cfa_", letters[c], "_", df_wide$vars_tmp[c], ".png"))
  
}


```

![](cfa_a_selfcon.png)
![](cfa_b_pint.png)
![](cfa_c_genselfeff.png)
![](cfa_d_efficacy_matched.png)



### student reports of teaching

```{r, echo = F, results='asis'}
# define constructs measures
constructs <- c(
  "Disruptions", 
  "Teacher’s classroom management", 
  "Teacher support for learning", 
  "Perceived support for competence", 
  "Perceived support for autonomy", 
  "Student-teacher relationship", 
  "Student participation in discourse", 
  "Clarity of instruction", 
  "Focus on meaning", 
  "Cognitive activation", 
  "Adaption of instruction",
  "Feedback"
)

# determine unique variable names
vars <- c(
  # CM disruption
  "DISRUPR_posttest",
  # CM teacher management
  "TEACHMAN_posttest",
  # SE teacher support
  "TESUP_posttest",
  # SE support for competence
  "SUPCOM_posttest",
  # SE support for autonomy
  "SUPAUT_posttest",
  # student teacher relationship 
  "REL_STUD_TEACH_posttest",
  # discourse
  "DISCOURSE_posttest",
  # clarity
  "CLARITY_posttest",
  # meaning
  "MEANING_posttest",
  # cognitive activation
  "COCACT_posttest",
  # adaptation of instruction
  "ADAPT_posttest",
  # feedback
  "FEEDBACK_posttest"
)

# add timepoint
timepoints <- rep_len(c("T2"), length.out = length(vars))

# create nice outcome names
vars_out <- paste(constructs, timepoints)

# determine items that are summed up to factor
scales <- c(
  # CM disruption
  paste0("SQB11", LETTERS[1:3],collapse = " + "),
  # CM teacher management
  paste0("SQB11", LETTERS[c(6, 7, 9, 10)],collapse = " + "),
  # SE teacher support
  paste0("SQB12", LETTERS[1:3],collapse = " + "),
  # SE support competence
  paste0("SQB12", LETTERS[4:7],collapse = " + "),
  # SE support autonomy
  paste0("SQB12", LETTERS[8:11],collapse = " + "),
  # student teacher relationship 
  paste0("SQB13", LETTERS[1:5],collapse = " + "),
  # discourse
  paste0("SQB08", LETTERS[9:11],collapse = " + "),
  # clarity
  paste0("SQB08", LETTERS[1:4],collapse = " + "),
  # meaning
  paste0("SQB09", LETTERS[1:4],collapse = " + "),
  # cognitive activation
  paste0("SQB08", LETTERS[5:8],collapse = " + "),
  # adaptation of instruction
  paste0("SQB10", LETTERS[1:5],collapse = " + "),
  # feedback
  paste0("SQB16", LETTERS[1:4],collapse = " + ")
)


# combine to df for checking
df <- data.frame(vars, constructs, timepoints, vars_out, scales)
df$models <- paste(timepoints, "=~", scales)


```


```{r, echo = F, results='asis', fig.align='center', warning=FALSE}

# determine file name for table in APA

filename <- "01_mi_tq.csv"

# run chunk
<<working_chunk>>

```



##### Confirmatory factor analysis (CFA) plots for the non-cognitive outcomes  

```{r, echo = F, results='asis', fig.align='center', warning=FALSE}
# create all CFA plots #

# format input data
df$vars_tmp <- tolower(gsub("_baseline|_posttest", "", df$vars))

# long to wide
df_wide <- df %>%
  select(vars_tmp, constructs, timepoints, models) %>%
  tidyr::pivot_wider(names_from = timepoints, values_from = models)

# for each construct
for (c in 1:length(df_wide$constructs)) {
  
  # create plot
  create_cfa_plot(model = df_wide$T2[c],
                  construct_name = paste(LETTERS[c], df_wide$constructs[c] ),
                  filename = paste0("cfa_", letters[c], "_", df_wide$vars_tmp[c], ".png"))
  
}


```

```{r, echo = F, results='asis', fig.align='center', warning=FALSE}
# define constructs measures
constructs <- c(
  "Teaching quality"
)

# determine unique variable names
vars <- c(
  # teaching quality
  "TQ_posttest"
)

# add timepoint
timepoints <- "T2"

# create nice outcome names
vars_out <- paste(constructs, timepoints)

# determine items that are summed up to factor
scales <-  ' 
  f1 =~ SQB08A + SQB08B + SQB08C + SQB08D
  f2 =~ SQB08E + SQB08F + SQB08G + SQB08H
  f3 =~ SQB08I + SQB08J + SQB08K
  f4 =~ SQB09A + SQB09B + SQB09C + SQB09D
  f5 =~ SQB10A + SQB10B + SQB10C + SQB10D + SQB10E
  f6 =~ SQB11A + SQB11B + SQB11C
  f7 =~ SQB11F + SQB11G + SQB11I + SQB11J
  f8 =~ SQB12A + SQB12B + SQB12C
  f9 =~ SQB12D + SQB12E + SQB12F + SQB12G 
  f10 =~ SQB12H + SQB12I + SQB12J + SQB12K 
  f11 =~ SQB13A + SQB13B + SQB13C + SQB13D + SQB13E 
  f12 =~ SQB16A + SQB16B + SQB16C + SQB16D 
'


# combine to df for checking
df <- data.frame(vars, constructs, timepoints, vars_out, scales)
df$models <- scales



```

```{r, echo = F, results='asis', fig.align='center', warning=FALSE}

# determine file name for table in APA

filename <- "01_mi_mgti.csv"

# run chunk
<<working_chunk>>

```
